{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeds1219/VQA_retriever/blob/main/MAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install higher transformers torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkqt8VeussLI",
        "outputId": "0dab9970-f0a6-4fd2-f7fb-d114abb13605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting higher\n",
            "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: higher\n",
            "Successfully installed higher-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "rj5-6cYxyTck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPEmbedder:\n",
        "    def __init__(self, model_name=\"openai/clip-vit-base-patch16\", freeze_question=True, freeze_image_model=True):\n",
        "        self.model = CLIPModel.from_pretrained(model_name)\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        self.projection_layer = nn.Linear(1024, 512)\n",
        "        if freeze_image_model:\n",
        "            for param in self.model.vision_model.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"Image model is frozen.\")\n",
        "\n",
        "        self.question_model = CLIPModel.from_pretrained(model_name)\n",
        "        if freeze_question:\n",
        "            for param in self.question_model.text_model.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"Question model is frozen.\")\n",
        "\n",
        "\n",
        "        self.caption_model = CLIPModel.from_pretrained(model_name)\n",
        "        for param in self.caption_model.text_model.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"Caption model is trainable.\")\n",
        "\n",
        "    def get_question_embedding(self, question):\n",
        "\n",
        "        question_inputs = self.processor(text=question, return_tensors=\"pt\", padding=True)\n",
        "        question_features = self.question_model.get_text_features(**question_inputs)\n",
        "        return question_features\n",
        "\n",
        "    def get_caption_embedding(self, caption):\n",
        "\n",
        "        caption_inputs = self.processor(text=caption, return_tensors=\"pt\", padding=True)\n",
        "        caption_features = self.caption_model.get_text_features(**caption_inputs)\n",
        "        return caption_features\n",
        "\n",
        "    def get_image_embedding(self, images):\n",
        "\n",
        "        image_inputs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "        image_features = self.model.get_image_features(**image_inputs)\n",
        "        return image_features\n",
        "\n",
        "    def cosine_similarity(self, embedding1, embedding2):\n",
        "\n",
        "        embedding1_norm = embedding1 / embedding1.norm(dim=-1, keepdim=True)\n",
        "        embedding2_norm = embedding2 / embedding2.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarity = (embedding1_norm * embedding2_norm).sum(dim=-1)\n",
        "        return similarity\n",
        "\n",
        "    def compute_query_embedding(self, questions, images):\n",
        "\n",
        "        question_embedding = self.get_question_embedding(questions)\n",
        "        image_embedding = self.get_image_embedding(images)\n",
        "\n",
        "        combined_embedding = torch.cat((question_embedding, image_embedding), dim=-1)\n",
        "\n",
        "        query_embedding = self.projection_layer(combined_embedding)\n",
        "        return query_embedding\n",
        "\n",
        "#################################Did not review###################################################################\n",
        "    def contrastive_loss(self, query_embeddings, caption_embeddings, margin=0.2):\n",
        "\n",
        "        query_embeddings = query_embeddings / query_embeddings.norm(dim=-1, keepdim=True)\n",
        "        caption_embeddings = caption_embeddings / caption_embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarities = torch.matmul(query_embeddings, caption_embeddings.T)\n",
        "\n",
        "        positive_similarities = torch.diag(similarities)\n",
        "\n",
        "        batch_size = similarities.size(0)\n",
        "        negative_loss = torch.sum(F.relu(margin + similarities - positive_similarities.unsqueeze(1)))\n",
        "        positive_loss = torch.sum(1 - positive_similarities)\n",
        "        loss = (positive_loss + negative_loss) / batch_size\n",
        "\n",
        "        return loss\n",
        "\n",
        "#################################Did not review###################################################################\n",
        "    def train_maml(self, tasks, adaptation_steps=1, meta_lr=1e-3, inner_lr=1e-2):\n",
        "        meta_optimizer = Adam(self.caption_model.parameters(), lr=meta_lr)\n",
        "\n",
        "        for epoch in range(10):\n",
        "            meta_loss = 0.0\n",
        "            for task in tasks:\n",
        "                # task는 이제 하나의 리스트로 되어 있고, 이를 반으로 나누어 support set과 query set으로 분리\n",
        "                split_idx = len(task) // 2\n",
        "                support_set = task[:split_idx]  # 앞부분을 support set으로\n",
        "                query_set = task[split_idx:]  # 뒷부분을 query set으로\n",
        "\n",
        "                # Support set에서 질문, 이미지, 캡션을 추출\n",
        "                support_questions, support_images, support_captions = zip(*support_set)\n",
        "                # Query set에서 질문, 이미지, 캡션을 추출\n",
        "                query_questions, query_images, query_captions = zip(*query_set)\n",
        "\n",
        "                self.caption_model.train()\n",
        "                adapted_params = list(self.caption_model.parameters())\n",
        "\n",
        "                # Adaptation steps (훈련용 데이터로 모델 적응)\n",
        "                for step in range(adaptation_steps):\n",
        "                    print(f\"\\nAdaptation step {step+1}\")\n",
        "\n",
        "                    # Support set을 사용하여 임베딩 계산\n",
        "                    query_embeddings = self.compute_query_embedding(support_questions, support_images)\n",
        "                    caption_embeddings = self.get_caption_embedding(support_captions)\n",
        "\n",
        "                    # Contrastive loss 계산\n",
        "                    loss = self.contrastive_loss(query_embeddings, caption_embeddings)\n",
        "\n",
        "                    # Gradient 계산 후 모델 파라미터 업데이트\n",
        "                    loss.backward()\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        for param in adapted_params:\n",
        "                            if param.grad is not None:\n",
        "                                param.data -= inner_lr * param.grad.data\n",
        "\n",
        "                    self.caption_model.zero_grad()\n",
        "\n",
        "                # Query set을 사용하여 손실 계산 (평가)\n",
        "                query_embeddings = self.compute_query_embedding(query_questions, query_images)\n",
        "                caption_embeddings = self.get_caption_embedding(query_captions)\n",
        "                task_loss = self.contrastive_loss(query_embeddings, caption_embeddings)\n",
        "                meta_loss += task_loss\n",
        "\n",
        "            # Meta optimization step (전체 task에 대해 손실을 최소화)\n",
        "            meta_optimizer.zero_grad()\n",
        "            meta_loss.backward()\n",
        "            meta_optimizer.step()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}, Meta Loss: {meta_loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cgEzojl5zavP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Example batch of tasks\n",
        "tasks = [\n",
        "    # 첫 번째 task\n",
        "    [\n",
        "        # Support set: 첫 번째 2개의 샘플\n",
        "        (\"What is in the image?\", Image.open(\"/content/image1.jpg\"), \"A cat on a sofa.\"),\n",
        "        (\"What is in the image?\", Image.open(\"/content/image2.jpg\"), \"A dog in the park.\"),\n",
        "        # Query set: 나머지 2개의 샘플\n",
        "        (\"What is in the image?\", Image.open(\"/content/image3.jpg\"), \"A bird flying.\"),\n",
        "        (\"What is in the image?\", Image.open(\"/content/image4.jpg\"), \"A car on the road.\")\n",
        "    ],\n",
        "    # 두 번째 task\n",
        "    [\n",
        "        # Support set: 첫 번째 2개의 샘플\n",
        "        (\"Describe the scene.\", Image.open(\"/content/image5.jpg\"), \"A lake at sunset.\"),\n",
        "        (\"Describe the scene.\", Image.open(\"/content/image6.jpg\"), \"A mountain covered with snow.\"),\n",
        "        # Query set: 나머지 2개의 샘플\n",
        "        (\"Describe the scene.\", Image.open(\"/content/image7.jpg\"), \"A city skyline.\"),\n",
        "        (\"Describe the scene.\", Image.open(\"/content/image8.jpg\"), \"A desert with a camel.\")\n",
        "    ]\n",
        "]\n"
      ],
      "metadata": {
        "id": "uvwBXjVmG0A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = CLIPEmbedder()\n",
        "embedder.train_maml(tasks, adaptation_steps=5, meta_lr=1e-3, inner_lr=1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI3Tg7X7Lr9G",
        "outputId": "6f0d86cb-6d5b-4119-d171-c8b31edb0e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image model is frozen.\n",
            "Question model is frozen.\n",
            "Caption model is trainable.\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 1, Meta Loss: 1.6934982538223267\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 2, Meta Loss: 1.0984865427017212\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 3, Meta Loss: 1.0045394897460938\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 4, Meta Loss: 0.9734621047973633\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 5, Meta Loss: 0.9630635380744934\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 6, Meta Loss: 0.951380729675293\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 7, Meta Loss: 0.9381471872329712\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 8, Meta Loss: 0.9147927761077881\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 9, Meta Loss: 0.8246927261352539\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "\n",
            "Adaptation step 1\n",
            "\n",
            "Adaptation step 2\n",
            "\n",
            "Adaptation step 3\n",
            "\n",
            "Adaptation step 4\n",
            "\n",
            "Adaptation step 5\n",
            "Epoch 10, Meta Loss: 0.9217207431793213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBLxMOHxTLPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNImBB2bDEnqvBja8e4Yr1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}