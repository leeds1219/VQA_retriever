{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeds1219/VQA_retriever/blob/main/MAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install higher transformers torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkqt8VeussLI",
        "outputId": "0dab9970-f0a6-4fd2-f7fb-d114abb13605"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting higher\n",
            "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: higher\n",
            "Successfully installed higher-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "rj5-6cYxyTck"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"/content/sample.jpg\")"
      ],
      "metadata": {
        "id": "EQxtgc_8z42b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
        "image_features = model.get_image_features(**inputs)\n",
        "\n",
        "# print(image_features)"
      ],
      "metadata": {
        "id": "1sbtsciuz4N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Why do dogs wag their tails?\""
      ],
      "metadata": {
        "id": "RaX6LP05ye36"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_inputs = processor(text=[question], return_tensors=\"pt\", padding=True)\n",
        "text_features = model.get_text_features(**text_inputs)\n",
        "\n",
        "# print(text_features)"
      ],
      "metadata": {
        "id": "Q99FLDsWzAKZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions = [\n",
        "    \"Dogs wag their tails to show happiness, excitement, or to get attention.\",\n",
        "    \"The weather today is absolutely beautiful.\",\n",
        "    \"I love ending the day with a good book.\",\n",
        "    \"I long for peaceful moments in nature.\"\n",
        "]"
      ],
      "metadata": {
        "id": "USxilBbrzw4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_inputs = caption_processor(text=captions, return_tensors=\"pt\", padding=True)\n",
        "text_features = caption_model.get_text_features(**text_inputs)\n",
        "\n",
        "# print(text_features)"
      ],
      "metadata": {
        "id": "HpxzCQ1NzJ7e"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPEmbedder:\n",
        "    def __init__(self, model_name=\"openai/clip-vit-base-patch16\", freeze_question=True, freeze_image_model=True):\n",
        "        self.model = CLIPModel.from_pretrained(model_name)\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        self.projection_layer = nn.Linear(1024, 512)\n",
        "        if freeze_image_model:\n",
        "            for param in self.model.vision_model.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"Image model is frozen.\")\n",
        "\n",
        "        self.question_model = CLIPModel.from_pretrained(model_name)\n",
        "        if freeze_question:\n",
        "            for param in self.question_model.text_model.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"Question model is frozen.\")\n",
        "\n",
        "\n",
        "        self.caption_model = CLIPModel.from_pretrained(model_name)\n",
        "        for param in self.caption_model.text_model.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"Caption model is trainable.\")\n",
        "\n",
        "    def get_question_embedding(self, question):\n",
        "\n",
        "        question_inputs = self.processor(text=question, return_tensors=\"pt\", padding=True)\n",
        "        question_features = self.question_model.get_text_features(**question_inputs)\n",
        "        return question_features\n",
        "\n",
        "    def get_caption_embedding(self, caption):\n",
        "\n",
        "        caption_inputs = self.processor(text=caption, return_tensors=\"pt\", padding=True)\n",
        "        caption_features = self.caption_model.get_text_features(**caption_inputs)\n",
        "        return caption_features\n",
        "\n",
        "    def get_image_embedding(self, images):\n",
        "\n",
        "        image_inputs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "        image_features = self.model.get_image_features(**image_inputs)\n",
        "        return image_features\n",
        "\n",
        "    def cosine_similarity(self, embedding1, embedding2):\n",
        "\n",
        "        embedding1_norm = embedding1 / embedding1.norm(dim=-1, keepdim=True)\n",
        "        embedding2_norm = embedding2 / embedding2.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarity = (embedding1_norm * embedding2_norm).sum(dim=-1)\n",
        "        return similarity\n",
        "\n",
        "    def compute_query_embedding(self, questions, images):\n",
        "\n",
        "        question_embedding = self.get_question_embedding(questions)\n",
        "        image_embedding = self.get_image_embedding(images)\n",
        "\n",
        "        combined_embedding = torch.cat((question_embedding, image_embedding), dim=-1)\n",
        "\n",
        "        query_embedding = self.projection_layer(combined_embedding)\n",
        "        return query_embedding\n",
        "\n",
        "    def contrastive_loss(self, query_embeddings, caption_embeddings, margin=0.2):\n",
        "\n",
        "        query_embeddings = query_embeddings / query_embeddings.norm(dim=-1, keepdim=True)\n",
        "        caption_embeddings = caption_embeddings / caption_embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarities = torch.matmul(query_embeddings, caption_embeddings.T)\n",
        "\n",
        "        positive_similarities = torch.diag(similarities)\n",
        "\n",
        "        batch_size = similarities.size(0)\n",
        "        negative_loss = torch.sum(F.relu(margin + similarities - positive_similarities.unsqueeze(1)))\n",
        "        positive_loss = torch.sum(1 - positive_similarities)\n",
        "        loss = (positive_loss + negative_loss) / batch_size\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, questions, images, captions):\n",
        "\n",
        "        query_embeddings = self.compute_query_embedding(questions, images)\n",
        "\n",
        "        caption_embeddings = self.get_caption_embedding(captions)\n",
        "\n",
        "        loss = self.contrastive_loss(query_embeddings, caption_embeddings)\n",
        "        return loss\n",
        "\n",
        "    def train_maml(self, tasks, adaptation_steps=1, meta_lr=1e-3, inner_lr=1e-2):\n",
        "\n",
        "        meta_optimizer = Adam(self.parameters(), lr=meta_lr)\n",
        "\n",
        "        for epoch in range(10):\n",
        "            meta_loss = 0.0\n",
        "            for task in tasks:\n",
        "\n",
        "                questions, images, captions = task\n",
        "\n",
        "                adapted_params = {name: param.clone() for name, param in self.named_parameters()}\n",
        "\n",
        "                for _ in range(adaptation_steps):\n",
        "                    query_embeddings = self.compute_query_embedding(questions, images)\n",
        "                    caption_embeddings = self.get_caption_embedding(captions)\n",
        "                    loss = self.contrastive_loss(query_embeddings, caption_embeddings)\n",
        "\n",
        "                    grads = torch.autograd.grad(loss, adapted_params.values(), create_graph=True)\n",
        "                    adapted_params = {name: param - inner_lr * grad for (name, param), grad in zip(adapted_params.items(), grads)}\n",
        "\n",
        "                query_embeddings = self.compute_query_embedding(questions, images)\n",
        "                caption_embeddings = self.get_caption_embedding(captions)\n",
        "                task_loss = self.contrastive_loss(query_embeddings, caption_embeddings)\n",
        "                meta_loss += task_loss\n",
        "\n",
        "            meta_optimizer.zero_grad()\n",
        "            meta_loss.backward()\n",
        "            meta_optimizer.step()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}, Meta Loss: {meta_loss.item()}\")"
      ],
      "metadata": {
        "id": "cgEzojl5zavP"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_embedder = CLIPEmbedder(freeze_question=True, freeze_image_model=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlR-6hUm06vJ",
        "outputId": "9c257bc8-aafa-45ac-f196-2713c0f4b7d0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image model is frozen.\n",
            "Question model is frozen.\n",
            "Caption model is trainable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_embedding = clip_embedder.get_question_embedding(question)"
      ],
      "metadata": {
        "id": "C8C5Zk-t09yR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption = captions[0]"
      ],
      "metadata": {
        "id": "Lnv15e2E1BFd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_embedding = clip_embedder.get_caption_embedding(caption)"
      ],
      "metadata": {
        "id": "VmpFka-p0-2m"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_embedding = clip_embedder.get_image_embedding(image)"
      ],
      "metadata": {
        "id": "fWlisOt31FsC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoEIPLZ91QA2",
        "outputId": "ee3694b0-5b53-491a-ee7b-adf6e4838897"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caption_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08TWmkso1MHc",
        "outputId": "f731c10e-a336-4b27-998a-19a177f7221c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVEbf9aT1PDk",
        "outputId": "0ea6f4a1-4112-4067-ad07-be96cd1c304a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = torch.cat((question_embedding, image_embedding), dim=1)"
      ],
      "metadata": {
        "id": "mQ7S0vQA1SHs"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gllnxOiA1lrS",
        "outputId": "df7518d6-a8cf-47eb-a843-36a36f2e45b6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "projection_layer = nn.Linear(1024, 512)\n",
        "query_embedding_projected = projection_layer(query_embedding)"
      ],
      "metadata": {
        "id": "pkWMjhOB1nAD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding_projected.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yujd7Lc-2JMe",
        "outputId": "caa5da55-ace9-41dc-d348-562243050181"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "query_embedding_projected = torch.rand(1, 512)  # (1, 512)\n",
        "caption_embedding = torch.rand(1, 512)  # (1, 512)\n",
        "\n",
        "cosine_similarity = clip_embedder.cosine_similarity(query_embedding_projected, caption_embedding)\n",
        "\n",
        "print(f\"Cosine Similarity: {cosine_similarity.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG3AAVPt2LPL",
        "outputId": "9e242361-177b-4179-a9d8-d111c9edd62b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.7582837343215942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_caption_embedding = clip_embedder.get_caption_embedding(captions[-1])"
      ],
      "metadata": {
        "id": "XmoNBErh2Uw1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_cosine_similarity = clip_embedder.cosine_similarity(query_embedding_projected, negative_caption_embedding)"
      ],
      "metadata": {
        "id": "9r8vo9XPD5Ix"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Negative Cosine Similarity: {negative_cosine_similarity.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k7Yv5UXD_GN",
        "outputId": "5461ca41-aedc-4cb1-ab45-0f86eb9131b0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative Cosine Similarity: -0.002463156823068857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "c_NBt8dPG9rP"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "embedder = CLIPEmbedder()\n",
        "\n",
        "# Example data\n",
        "questions = [\"What is in the image?\", \"Describe the scene.\"]\n",
        "images = [Image.open(\"/content/sample_dog.jpg\"), Image.open(\"/content/sample_cat.jpeg\")]\n",
        "captions = [\"A cat sitting on a sofa.\", \"A dog running in a park.\"]\n",
        "\n",
        "# Perform a training step\n",
        "loss = embedder.train_step(questions, images, captions)\n",
        "print(f\"Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNZNFkPzEA-d",
        "outputId": "ce8a9b9a-b9a8-4854-b60d-99aae0156100"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image model is frozen.\n",
            "Question model is frozen.\n",
            "Caption model is trainable.\n",
            "Loss: 1.3966864347457886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uvwBXjVmG0A_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI7eI/qjtTpUVn/3tXZOTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}