{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeds1219/VQA_retriever/blob/main/MAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install higher transformers torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkqt8VeussLI",
        "outputId": "0dab9970-f0a6-4fd2-f7fb-d114abb13605"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting higher\n",
            "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: higher\n",
            "Successfully installed higher-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import higher  # MAML을 위한 라이브러리\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "class CaptionSearchModel(nn.Module):\n",
        "    def __init__(self, pretrained_model=\"openai/clip-vit-base-patch32\"):\n",
        "        super(CaptionSearchModel, self).__init__()\n",
        "        # CLIP 모델 로드\n",
        "        self.clip = CLIPModel.from_pretrained(pretrained_model)\n",
        "        self.processor = CLIPProcessor.from_pretrained(pretrained_model)\n",
        "\n",
        "        # CLIP 모델의 모든 파라미터를 frozen 상태로 설정 (이미지 인코더와 텍스트 인코더 포함)\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # 학습 가능한 캡션 텍스트 인코더\n",
        "        # 캡션 텍스트 인코더는 텍스트 모델을 그대로 사용하되, 이 부분은 학습이 가능하도록 설정\n",
        "        self.caption_text_encoder = self.clip.text_model\n",
        "        for param in self.caption_text_encoder.parameters():\n",
        "            param.requires_grad = True  # 텍스트 인코더는 학습 가능\n",
        "\n",
        "    def forward(self, text_inputs, image):\n",
        "        # CLIP을 사용해 텍스트와 이미지 임베딩 생성 (학습되지 않음)\n",
        "        inputs = self.processor(text=text_inputs, images=image, return_tensors=\"pt\", padding=True)\n",
        "        outputs = self.clip(**inputs)\n",
        "\n",
        "        text_embedding = outputs.text_embeds  # 텍스트 임베딩\n",
        "        image_embedding = outputs.image_embeds  # 이미지 임베딩\n",
        "\n",
        "        # 텍스트와 이미지 임베딩을 concat\n",
        "        combined_embedding = torch.cat((text_embedding, image_embedding), dim=-1)\n",
        "        return combined_embedding\n",
        "\n",
        "    def compute_similarity(self, query_embedding, caption_embedding):\n",
        "        \"\"\"\n",
        "        주어진 query 임베딩과 caption 임베딩 간의 유사도 계산\n",
        "        \"\"\"\n",
        "        query_embedding = F.normalize(query_embedding, p=2, dim=-1)\n",
        "        caption_embedding = F.normalize(caption_embedding, p=2, dim=-1)\n",
        "\n",
        "        similarity = torch.matmul(query_embedding, caption_embedding.T)\n",
        "        return similarity\n",
        "\n",
        "\n",
        "# Contrastive Loss 정의\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, positive_similarity, negative_similarity):\n",
        "        \"\"\"\n",
        "        Contrastive loss 계산\n",
        "        \"\"\"\n",
        "        loss_pos = torch.clamp(positive_similarity, min=0)\n",
        "        loss_neg = torch.clamp(self.margin - negative_similarity, min=0)\n",
        "        loss = loss_pos + loss_neg\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# 학습기 정의\n",
        "class CaptionSearchTrainer:\n",
        "    def __init__(self, model, tokenizer, lr=1e-5, margin=1.0):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        self.criterion = ContrastiveLoss(margin=margin)\n",
        "\n",
        "    def train_step(self, query, image, positive_caption, negative_captions):\n",
        "        # 텍스트와 이미지 임베딩을 생성\n",
        "        query_embedding = self.model(query, image)\n",
        "\n",
        "        # Positive Caption 임베딩 생성\n",
        "        positive_caption_embedding = self.model(positive_caption, image)\n",
        "\n",
        "        # Negative Caption 임베딩 생성 (배치 내 다른 캡션들은 negative)\n",
        "        negative_caption_embeddings = []\n",
        "        for neg_caption in negative_captions:\n",
        "            negative_caption_embeddings.append(self.model(neg_caption, image))\n",
        "        negative_caption_embeddings = torch.stack(negative_caption_embeddings, dim=0)\n",
        "\n",
        "        # Similarity 계산\n",
        "        positive_similarity = self.model.compute_similarity(query_embedding, positive_caption_embedding)\n",
        "\n",
        "        # Negative similarity는 배치 내의 다른 샘플들과 비교\n",
        "        negative_similarities = []\n",
        "        for neg_embedding in negative_caption_embeddings:\n",
        "            negative_similarities.append(self.model.compute_similarity(query_embedding, neg_embedding))\n",
        "\n",
        "        negative_similarities = torch.stack(negative_similarities, dim=0)\n",
        "\n",
        "        # Contrastive loss 계산\n",
        "        loss = self.criterion(positive_similarity, negative_similarities)\n",
        "        return loss\n",
        "\n",
        "    def meta_train_step(self, support_query, support_image, support_positive_caption, support_negative_captions, query_query, query_image, query_positive_caption, query_negative_captions):\n",
        "        \"\"\"\n",
        "        MAML의 inner loop와 outer loop를 구현하는 함수\n",
        "        \"\"\"\n",
        "        # higher 라이브러리를 사용하여 모델 파라미터를 복사합니다.\n",
        "        # copy_initial_weights=True는 모델 파라미터를 복사하여 inner loop에서 업데이트할 수 있도록 합니다.\n",
        "        with higher.innerloop_ctx(self.model, self.optimizer, copy_initial_weights=True) as (fmodel, diffopt):\n",
        "            # Support set을 사용하여 모델을 학습 (inner loop)\n",
        "            support_loss = self.train_step(support_query, support_image, support_positive_caption, support_negative_captions)\n",
        "            diffopt.step(support_loss)  # inner loop에서 gradient 업데이트\n",
        "\n",
        "            # Query set을 사용하여 평가 (outer loop)\n",
        "            query_loss = self.train_step(query_query, query_image, query_positive_caption, query_negative_captions)\n",
        "\n",
        "        return query_loss\n",
        "\n",
        "    def train(self, dataloader, num_epochs=10):\n",
        "        self.model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            for batch in dataloader:\n",
        "                # 배치를 Support set과 Query set으로 나눕니다\n",
        "                support_query, support_image, support_positive_caption, support_negative_captions, query_query, query_image, query_positive_caption, query_negative_captions = batch\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # MAML을 통한 메타 학습\n",
        "                loss = self.meta_train_step(support_query, support_image, support_positive_caption, support_negative_captions, query_query, query_image, query_positive_caption, query_negative_captions)\n",
        "\n",
        "                loss.backward()  # gradient 계산\n",
        "                self.optimizer.step()  # 메타 학습을 위한 파라미터 업데이트\n",
        "\n",
        "                print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "# 예시 데이터 (실제로는 데이터셋을 불러와야 함)\n",
        "captions = [\"A man is holding a dog\", \"A car is parked on the road\", \"A person is riding a bicycle\", \"A dog is playing in the park\"]\n",
        "question = \"What is the person doing?\"\n",
        "image = torch.randn(1, 3, 224, 224)  # 가상의 이미지 Tensor\n",
        "\n",
        "# 모델 초기화\n",
        "model = CaptionSearchModel()\n",
        "\n",
        "# 모델과 토크나이저, 학습기 초기화\n",
        "tokenizer = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "trainer = CaptionSearchTrainer(model, tokenizer)\n",
        "\n",
        "# 가상의 데이터 로더\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "max_length = 77  # CLIP 모델에서 사용하는 최대 길이 (텍스트 길이를 77로 고정)\n",
        "\n",
        "# Tokenizer를 사용하여 텍스트를 토큰화한 뒤, 텐서로 변환 (padding과 truncation 적용)\n",
        "query_tensor = tokenizer(question, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=max_length)\n",
        "positive_tensor = tokenizer(captions[0], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=max_length)\n",
        "negative_tensor = [tokenizer(caption, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=max_length) for caption in captions[1:]]\n",
        "\n",
        "# 이미지는 Tensor로 변환하여 사용\n",
        "image_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Dataset과 DataLoader 생성\n",
        "dataset = TensorDataset(query_tensor['input_ids'], image_tensor, positive_tensor['input_ids'],\n",
        "                        torch.stack([neg['input_ids'] for neg in negative_tensor], dim=0))\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "# 학습 시작\n",
        "trainer.train(dataloader, num_epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "OAwrQMyhTRVa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_tensor"
      ],
      "metadata": {
        "id": "iShY0vjDdX98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e15c8b-4f03-4cf9-ad5c-710a966fa7ce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[49406,   768,   533,   518,  2533,  1960,   286, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNkwwc_ruRD9",
        "outputId": "bd461ef5-108a-4e97-a3bd-d144d3b9999e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[49406,   320,   786,   533,  5050,   320,  1929, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfP_nkfUuU-S",
        "outputId": "a4ef63c5-86b8-4eea-cd19-ccf87c4f2d50"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input_ids': tensor([[49406,   320,  1615,   533, 16487,   525,   518,  1759, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0]])},\n",
              " {'input_ids': tensor([[49406,   320,  2533,   533,  6765,   320, 11652, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0]])},\n",
              " {'input_ids': tensor([[49406,   320,  1929,   533,  1629,   530,   518,  1452, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "          49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0]])}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_tensor['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbGvDGRRuXe8",
        "outputId": "bdf2db8e-3155-431f-b8f4-7eb6983b71ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49406,   768,   533,   518,  2533,  1960,   286, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tensor['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OT8lTy3ue3b",
        "outputId": "b9c98697-4fcf-4ba1-81da-033d4a361b30"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49406,   320,   786,   533,  5050,   320,  1929, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
              "         49407, 49407, 49407, 49407, 49407, 49407, 49407]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "562fOZ-juiB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3SYj2D5VTjibtF+hNlouf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}